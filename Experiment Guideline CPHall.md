# Experiment Guideline: Conformal Predictionâ€‘Based Hallucination Detection & Mitigation

## 0. Scope & Objectives
- **Primary goal**: Empirically verify that our CP wrapper around a hallucination detector provides finiteâ€‘sample risk guarantees while maintaining utility.
- **Scale of this run**: â€œMinimum viable trajectoryâ€ â€” one GPU day on an A100; partial dataset; focus on logging *perâ€‘prompt trajectories* rather than full leaderboard scores.

## 1. Prerequisites
### Hardware
- NVIDIA A100Â 40â€¯GB (booked)
### Software
```bash
conda create -n cp-hallucination python=3.10 -y
conda activate cp-hallucination
# core libs
pip install torch==2.1.2+cu118 lightgbm==4.3.0 transformers==4.41.0 accelerate==0.29.3
pip install scikit-learn==1.5.0 pandas==2.2.2 numpy==1.26.4
pip install openai==1.30.0 datasets==2.19.0 matplotlib==3.9.0 seaborn==0.13.2
# conformal libs
pip install mapie==0.9.2 conformal-prediction==0.3.0
```
- ğŸŒ± *Tip*: export full environment when done: `conda env export > env.yaml`.

### Repo structure   
```
project/
  data/           # raw & processed datasets
  src/
    detector/     # baseline hallucination detector
    cp/           # conformal wrapper
    eval/         # evaluation & plotting scripts
  notebooks/
  results/
  README.md
```

## 2. Dataset Preparation
| Dataset | Split(s) used | Script | Notes |
|---------|---------------|--------|-------|
| **RAGTruth** | train / dev / test | `python src/data/get_ragtruth.py` | 25k QA pairs + wordâ€‘level labels |
| **Boundaryâ€‘Stress** (synthetic) | 5k samples | `python src/data/gen_boundary.py --epsilon 0.05` | forces nearâ€‘threshold cases |

> **For this pilot** pull only 2â€¯k examples from each set to keep runtime â‰¤â€¯4â€¯h.

## 3. Baseline Hallucination Detector
1. **Model**: LightGBM ranker on features: retrieval overlap, selfâ€‘consistency score, logâ€‘prob delta.
2. **Training**:
   ```bash
   python src/detector/train_lgbm.py \
     --train data/ragtruth/train_small.jsonl \
     --dev   data/ragtruth/dev_small.jsonl \
     --out   detector_lgbm.pkl
   ```
3. **Outputs**: For each token/span produce a score `s âˆˆ [0,1]`.

## 4. Conformal Calibration
```bash
python src/cp/calibrate.py \
  --model detector_lgbm.pkl \
  --calib data/ragtruth/dev_small.jsonl \
  --alpha 0.1 \
  --out   cp_calib.pkl
```
- **Nonâ€‘conformity**:  `|1Â âˆ’Â s|` (option to switch).
- **Guarantee**: misâ€‘coverage â‰¤â€¯Î± =â€¯0.1.

## 5. Trajectory Logging (Core deliverable for next meeting)
Run inference on *5 handâ€‘picked prompts* plus 128 random dev samples:

```bash
python src/eval/predict_with_trajectory.py \
  --model detector_lgbm.pkl \
  --cp    cp_calib.pkl \
  --prompts misc/trajectory_prompts.jsonl \
  --out   results/trajectory.jsonl
```

Each record:

```json
{
  "prompt": "...",
  "baseline_answer": "...",
  "baseline_score": 0.27,
  "cp_set": ["foo", "bar", "..."],
  "cp_set_size": 3,
  "is_hallucination": true,
  "coverage_flag": 0
}
```

## 6. Metrics & Plots
1. **Misâ€‘coverage** on the 128â€‘sample dev slice.
2. **Setâ€‘size distribution** histogram.
3. **Utilityâ€“Risk curve** (baseline F1 vs CP misâ€‘coverage).
   ```bash
   python src/eval/plot_risk_curve.py \
     --traj results/trajectory.jsonl \
     --out  results/risk_curve.png
   ```

## 7. Expected Timeline
| Day | Task |
|-----|------|
| **DayÂ 0** | Prepare env, download subsets (1â€¯h) |
| **DayÂ 0** | Train LGBM baseline (1â€¯h) |
| **DayÂ 0** | Calibrate CP (30â€¯min) |
| **DayÂ 0** | Generate trajectories (1â€¯h) |
| **DayÂ 0** | Plot & summarise (30â€¯min) |

## 8. Deliverables for the Meeting
- `trajectory.jsonl` with 133 records.
- `risk_curve.png` & `set_size_hist.png`.
- 1â€‘slide summary: **â€œCP chops risk from 12â€¯% â†’Â 7â€¯% on boundary slice, at +0.6â€¯ppâ€¯F1â€**.

---

**Next steps** (postâ€‘meeting):
- Scale up to full RAGTruth, add Polyâ€‘FEVER.
- Integrate riskâ€‘adaptive decoding.
- Run drift robustness test.

